+ python -m seamlessm4t.finetune --src_lang bem --tgt_lang bem --train_dataset /scratch/cmeng2/datasets/iwslt2025-lres-st-os-input/bem-eng/bigc/asr.tc.punc.train.tsv --eval_dataset /scratch/cmeng2/datasets/iwslt2025-lres-st-os-input/bem-eng/bigc/asr.tc.punc.valid.tsv --model_name facebook/seamless-m4t-v2-large --save_model_to /scratch/cmeng2/experiments/iwslt2025-lres-st-os-seamless/bem-eng/bigc/ckpts/v2_ASR_tc-punc_lr1e-4_warmup1000_epoch10_bsz120_freq30_max30s_p10x250 --save_processor_path /scratch/cmeng2/experiments/iwslt2025-lres-st-os-seamless/bem-eng/bigc/ckpts/v2_ASR_tc-punc_lr1e-4_warmup1000_epoch10_bsz120_freq30_max30s_p10x250_proc --mode SPEECH_TO_TEXT --eval_mode SPEECH_TO_TEXT --learning_rate 1e-4 --warmup_steps 1000 --max_epochs 10 --batch_size 120 --update_freq 30 --max_speech_dur 30.0 --eval_steps 250 --patience 10 --log_steps 50 --dropout_fix
2025-05-22 16:20:18 | INFO | __main__ | input args: Namespace(src_lang='bem', tgt_lang='bem', train_dataset=PosixPath('/scratch/cmeng2/datasets/iwslt2025-lres-st-os-input/bem-eng/bigc/asr.tc.punc.train.tsv'), eval_dataset=PosixPath('/scratch/cmeng2/datasets/iwslt2025-lres-st-os-input/bem-eng/bigc/asr.tc.punc.valid.tsv'), model_name='facebook/seamless-m4t-v2-large', load_from_mt_pretrained=None, load_from_asr_pretrained=None, save_model_to=PosixPath('/scratch/cmeng2/experiments/iwslt2025-lres-st-os-seamless/bem-eng/bigc/ckpts/v2_ASR_tc-punc_lr1e-4_warmup1000_epoch10_bsz120_freq30_max30s_p10x250'), finetune_text_encoder=False, processor_path=None, save_processor_path='/scratch/cmeng2/experiments/iwslt2025-lres-st-os-seamless/bem-eng/bigc/ckpts/v2_ASR_tc-punc_lr1e-4_warmup1000_epoch10_bsz120_freq30_max30s_p10x250_proc', learning_rate=0.0001, alpha=1, beta=1, gamma=1, kd_loss_type=<KdLossType.KLD: 'KLD'>, detach_teacher=False, warmup_steps=1000, batch_size=120, update_freq=30, max_text_tokens=128, max_speech_dur=30.0, device='cuda', mode=<FinetuneMode.SPEECH_TO_TEXT: 'SPEECH_TO_TEXT'>, eval_mode=<FinetuneMode.SPEECH_TO_TEXT: 'SPEECH_TO_TEXT'>, patience=10, max_epochs=10, eval_steps=250, log_steps=50, seed=42, untie_lm_head=False, prefix_skip_len=0, dropout_fix=True)
2025-05-22 16:20:18 | INFO | __main__ | Set seed to 42
2025-05-22 16:20:18 | INFO | __main__ | Will use real batch size 4 with gradient accumulation 30
2025-05-22 16:20:18 | INFO | __main__ | The effective batch size is 120
2025-05-22 16:20:18 | INFO | __main__ | Finetune Params: FinetuneParams(model_name='facebook/seamless-m4t-v2-large', save_model_path=PosixPath('/scratch/cmeng2/experiments/iwslt2025-lres-st-os-seamless/bem-eng/bigc/ckpts/v2_ASR_tc-punc_lr1e-4_warmup1000_epoch10_bsz120_freq30_max30s_p10x250'), model_ver=<ModelVer.VER_2: 'VER_2'>, finetune_mode=<FinetuneMode.SPEECH_TO_TEXT: 'SPEECH_TO_TEXT'>, eval_mode=<FinetuneMode.SPEECH_TO_TEXT: 'SPEECH_TO_TEXT'>, finetune_text_encoder=False, train_batch_size=4, eval_batch_size=4, learning_rate=0.0001, alpha=1, beta=1, gamma=1, kd_loss_type=<KdLossType.KLD: 'KLD'>, detach_teacher=False, label_smoothing=0.2, prefix_skip_len=0, warmup_steps=1000, log_steps=50, eval_steps=250, update_freq=30, patience=10, max_epochs=10, float_dtype=torch.float16, device=device(type='cuda'))

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]
Loading checkpoint shards:  50%|█████     | 1/2 [00:00<00:00,  1.19it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.11it/s]
Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.12it/s]
2025-05-22 16:20:24 | INFO | seamlessm4t.model_utils | Will update decoder's ffn_dropout
2025-05-22 16:20:24 | INFO | seamlessm4t.model_utils | Will update adapter's attention score & ffn dropout
2025-05-22 16:20:24 | INFO | utils.model_utils | Model:
SeamlessM4Tv2Model(
  (shared): Embedding(256102, 1024, padding_idx=0)
  (text_encoder): None
  (speech_encoder): SeamlessM4Tv2SpeechEncoder(
    (feature_projection): SeamlessM4Tv2ConformerFeatureProjection(
      (layer_norm): LayerNorm((160,), eps=1e-05, elementwise_affine=True)
      (projection): Linear(in_features=160, out_features=1024, bias=True)
      (dropout): Dropout(p=0.0, inplace=False)
    )
    (encoder): SeamlessM4Tv2ConformerEncoder(
      (dropout): Dropout(p=0.0, inplace=False)
      (layers): ModuleList(
        (0-23): 24 x SeamlessM4Tv2ConformerEncoderLayer(
          (ffn1_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (ffn1): SeamlessM4Tv2ConformerFeedForward(
            (intermediate_dropout): Dropout(p=0.0, inplace=False)
            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): SiLU()
            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)
            (output_dropout): Dropout(p=0.0, inplace=False)
          )
          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (self_attn_dropout): Dropout(p=0.0, inplace=False)
          (self_attn): SeamlessM4Tv2ConformerSelfAttention(
            (linear_q): Linear(in_features=1024, out_features=1024, bias=True)
            (linear_k): Linear(in_features=1024, out_features=1024, bias=True)
            (linear_v): Linear(in_features=1024, out_features=1024, bias=True)
            (linear_out): Linear(in_features=1024, out_features=1024, bias=True)
            (dropout): Dropout(p=0.0, inplace=False)
            (distance_embedding): Embedding(73, 64)
          )
          (conv_module): SeamlessM4Tv2ConformerConvolutionModule(
            (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (pointwise_conv1): Conv1d(1024, 2048, kernel_size=(1,), stride=(1,), bias=False)
            (glu): GLU(dim=1)
            (depthwise_conv): Conv1d(1024, 1024, kernel_size=(31,), stride=(1,), groups=1024, bias=False)
            (depthwise_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
            (activation): SiLU()
            (pointwise_conv2): Conv1d(1024, 1024, kernel_size=(1,), stride=(1,), bias=False)
            (dropout): Dropout(p=0.0, inplace=False)
          )
          (ffn2_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (ffn2): SeamlessM4Tv2ConformerFeedForward(
            (intermediate_dropout): Dropout(p=0.0, inplace=False)
            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): SiLU()
            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)
            (output_dropout): Dropout(p=0.0, inplace=False)
          )
          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        )
      )
      (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
    )
    (intermediate_ffn): SeamlessM4Tv2ConformerFeedForward(
      (intermediate_dropout): Dropout(p=0.0, inplace=False)
      (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)
      (intermediate_act_fn): ReLU()
      (output_dense): Linear(in_features=4096, out_features=1024, bias=True)
      (output_dropout): Dropout(p=0.0, inplace=False)
    )
    (adapter): SeamlessM4Tv2ConformerAdapter(
      (layers): ModuleList(
        (0): SeamlessM4Tv2ConformerAdapterLayer(
          (residual_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (residual_conv): Conv1d(1024, 2048, kernel_size=(8,), stride=(8,), padding=(4,))
          (activation): GLU(dim=1)
          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (self_attn_conv): Conv1d(1024, 2048, kernel_size=(8,), stride=(8,), padding=(4,))
          (self_attn): SeamlessM4Tv2ConformerSelfAttention(
            (linear_q): Linear(in_features=1024, out_features=1024, bias=True)
            (linear_k): Linear(in_features=1024, out_features=1024, bias=True)
            (linear_v): Linear(in_features=1024, out_features=1024, bias=True)
            (linear_out): Linear(in_features=1024, out_features=1024, bias=True)
            (dropout): Dropout(p=0.1, inplace=False)
          )
          (self_attn_dropout): Dropout(p=0.1, inplace=False)
          (ffn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
          (ffn): SeamlessM4Tv2ConformerFeedForward(
            (intermediate_dropout): Dropout(p=0.0, inplace=False)
            (intermediate_dense): Linear(in_features=1024, out_features=4096, bias=True)
            (intermediate_act_fn): ReLU()
            (output_dense): Linear(in_features=4096, out_features=1024, bias=True)
            (output_dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
    )
    (inner_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  )
  (text_decoder): SeamlessM4Tv2Decoder(
    (embed_tokens): SeamlessM4Tv2ScaledWordEmbedding(256102, 1024, padding_idx=0)
    (embed_positions): SeamlessM4Tv2SinusoidalPositionalEmbedding()
    (layers): ModuleList(
      (0-23): 24 x SeamlessM4Tv2DecoderLayer(
        (self_attn): SeamlessM4Tv2Attention(
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (activation_fn): ReLU()
        (attn_dropout): Dropout(p=0.1, inplace=False)
        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (cross_attention): SeamlessM4Tv2Attention(
          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)
          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)
        )
        (cross_attention_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (ffn): SeamlessM4Tv2FeedForwardNetwork(
          (fc1): Linear(in_features=1024, out_features=8192, bias=True)
          (fc2): Linear(in_features=8192, out_features=1024, bias=True)
          (dropout): Dropout(p=0.0, inplace=False)
          (act): ReLU()
        )
        (ffn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
        (ffn_dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)
  )
  (lm_head): Linear(in_features=1024, out_features=256102, bias=False)
  (t2u_model): None
  (vocoder): None
)
2025-05-22 16:20:24 | INFO | utils.model_utils | Total num params: 1501.84M
2025-05-22 16:20:24 | INFO | utils.model_utils | Trainable num params: 1501.84M
2025-05-22 16:20:24 | INFO | __main__ | Load processor from facebook/seamless-m4t-v2-large
2025-05-22 16:20:27 | INFO | __main__ | Adding __bem__ to tokenizer
2025-05-22 16:20:27 | INFO | __main__ | Adding bem to model
2025-05-22 16:20:27 | INFO | __main__ | Save process to /scratch/cmeng2/experiments/iwslt2025-lres-st-os-seamless/bem-eng/bigc/ckpts/v2_ASR_tc-punc_lr1e-4_warmup1000_epoch10_bsz120_freq30_max30s_p10x250_proc
2025-05-22 16:20:29 | INFO | seamlessm4t.dataloader | Init data mode=FinetuneMode.SPEECH_TO_TEXT
2025-05-22 16:20:29 | INFO | seamlessm4t.dataloader | Batch config: BatchingConfig(max_text_tokens=128, max_speech_frames=480000, batch_size=4, num_workers=2, float_dtype=torch.float16)
2025-05-22 16:20:29 | INFO | seamlessm4t.dataloader | Init data mode=FinetuneMode.SPEECH_TO_TEXT
2025-05-22 16:20:29 | INFO | seamlessm4t.dataloader | Batch config: BatchingConfig(max_text_tokens=128, max_speech_frames=480000, batch_size=4, num_workers=1, float_dtype=torch.float16)
2025-05-22 16:20:41 | INFO | seamlessm4t.trainer | CalcLoss: 256102 classes neg=7.809388446790732e-07 | pos=0.8000007809388447 Kd Loss Type: KdLossType.KLD Detach Teacher: False prefix skip len: 0
2025-05-22 16:20:43 | INFO | seamlessm4t.trainer | Start Finetuning
2025-05-22 16:20:43 | INFO | seamlessm4t.trainer | Evaluation Step 0...
2025-05-22 16:22:06 | INFO | seamlessm4t.trainer | Eval after 0 updates: tot_loss=6.4459 best_loss=6.4459 patience_steps_left=10
2025-05-22 16:30:06 | INFO | seamlessm4t.trainer | Epoch 001 (686 steps) / update 50 total update 00050: train_loss=10.0853 last_lr=5.00E-06 peak_cuda_mem=60.74GB bsz=120.0 n_tokens=10199603.2 
2025-05-22 16:38:00 | INFO | seamlessm4t.trainer | Epoch 001 (686 steps) / update 100 total update 00100: train_loss=6.0453 last_lr=1.00E-05 peak_cuda_mem=57.36GB bsz=120.0 n_tokens=10094425.6 
2025-05-22 16:45:54 | INFO | seamlessm4t.trainer | Epoch 001 (686 steps) / update 150 total update 00150: train_loss=5.0133 last_lr=1.50E-05 peak_cuda_mem=55.46GB bsz=120.0 n_tokens=10092736.0 
2025-05-22 16:53:52 | INFO | seamlessm4t.trainer | Epoch 001 (686 steps) / update 200 total update 00200: train_loss=4.7058 last_lr=2.00E-05 peak_cuda_mem=57.58GB bsz=120.0 n_tokens=10141747.2 
2025-05-22 17:01:52 | INFO | seamlessm4t.trainer | Epoch 001 (686 steps) / update 250 total update 00250: train_loss=4.5383 last_lr=2.50E-05 peak_cuda_mem=58.65GB bsz=120.0 n_tokens=10165772.8 
2025-05-22 17:01:52 | INFO | seamlessm4t.trainer | Evaluation Step 1...
2025-05-22 17:03:09 | INFO | seamlessm4t.trainer | Eval after 250 updates: tot_loss=4.3585 best_loss=4.3585 patience_steps_left=10
2025-05-22 17:03:09 | INFO | seamlessm4t.trainer | Saving model
2025-05-22 17:11:22 | INFO | seamlessm4t.trainer | Epoch 001 (686 steps) / update 300 total update 00300: train_loss=4.4063 last_lr=3.00E-05 peak_cuda_mem=54.23GB bsz=120.0 n_tokens=9970918.4 
2025-05-22 17:19:27 | INFO | seamlessm4t.trainer | Epoch 001 (686 steps) / update 350 total update 00350: train_loss=4.2909 last_lr=3.50E-05 peak_cuda_mem=61.43GB bsz=120.0 n_tokens=10149043.2 
2025-05-22 17:27:29 | INFO | seamlessm4t.trainer | Epoch 001 (686 steps) / update 400 total update 00400: train_loss=4.1594 last_lr=4.00E-05 peak_cuda_mem=48.00GB bsz=120.0 n_tokens=10164224.0 
2025-05-22 17:35:30 | INFO | seamlessm4t.trainer | Epoch 001 (686 steps) / update 450 total update 00450: train_loss=4.0441 last_lr=4.50E-05 peak_cuda_mem=65.11GB bsz=120.0 n_tokens=10059724.8 
2025-05-22 17:43:32 | INFO | seamlessm4t.trainer | Epoch 001 (686 steps) / update 500 total update 00500: train_loss=3.9872 last_lr=5.00E-05 peak_cuda_mem=56.30GB bsz=120.0 n_tokens=10211353.6 
2025-05-22 17:43:32 | INFO | seamlessm4t.trainer | Evaluation Step 2...
2025-05-22 17:44:56 | INFO | seamlessm4t.trainer | Eval after 500 updates: tot_loss=3.8545 best_loss=3.8545 patience_steps_left=10
2025-05-22 17:44:56 | INFO | seamlessm4t.trainer | Saving model
2025-05-22 17:53:16 | INFO | seamlessm4t.trainer | Epoch 001 (686 steps) / update 550 total update 00550: train_loss=3.9128 last_lr=5.50E-05 peak_cuda_mem=52.76GB bsz=120.0 n_tokens=10219520.0 
2025-05-22 18:01:20 | INFO | seamlessm4t.trainer | Epoch 001 (686 steps) / update 600 total update 00600: train_loss=3.8552 last_lr=6.00E-05 peak_cuda_mem=65.32GB bsz=120.0 n_tokens=10080768.0 
2025-05-22 18:09:31 | INFO | seamlessm4t.trainer | Epoch 001 (686 steps) / update 650 total update 00650: train_loss=3.7862 last_lr=6.50E-05 peak_cuda_mem=65.02GB bsz=120.0 n_tokens=10119257.6 
2025-05-22 18:17:41 | INFO | seamlessm4t.trainer | Epoch 002 (686 steps) / update 14 total update 00700: train_loss=3.7519 last_lr=7.00E-05 peak_cuda_mem=46.38GB bsz=120.0 n_tokens=10221494.4 
2025-05-22 18:25:52 | INFO | seamlessm4t.trainer | Epoch 002 (686 steps) / update 64 total update 00750: train_loss=3.7162 last_lr=7.50E-05 peak_cuda_mem=61.29GB bsz=120.0 n_tokens=10179340.8 
2025-05-22 18:25:52 | INFO | seamlessm4t.trainer | Evaluation Step 3...
2025-05-22 18:27:15 | INFO | seamlessm4t.trainer | Eval after 750 updates: tot_loss=3.6679 best_loss=3.6679 patience_steps_left=10
2025-05-22 18:27:15 | INFO | seamlessm4t.trainer | Saving model
2025-05-22 18:35:39 | INFO | seamlessm4t.trainer | Epoch 002 (686 steps) / update 114 total update 00800: train_loss=3.6922 last_lr=8.00E-05 peak_cuda_mem=60.20GB bsz=120.0 n_tokens=10133222.4 
2025-05-22 18:43:55 | INFO | seamlessm4t.trainer | Epoch 002 (686 steps) / update 164 total update 00850: train_loss=3.6838 last_lr=8.50E-05 peak_cuda_mem=66.23GB bsz=120.0 n_tokens=10185228.8 
2025-05-22 18:52:03 | INFO | seamlessm4t.trainer | Epoch 002 (686 steps) / update 214 total update 00900: train_loss=3.6788 last_lr=9.00E-05 peak_cuda_mem=53.51GB bsz=120.0 n_tokens=9994944.0 
2025-05-22 19:00:20 | INFO | seamlessm4t.trainer | Epoch 002 (686 steps) / update 264 total update 00950: train_loss=3.6729 last_lr=9.50E-05 peak_cuda_mem=56.79GB bsz=120.0 n_tokens=10157542.4 
2025-05-22 19:08:35 | INFO | seamlessm4t.trainer | Epoch 002 (686 steps) / update 314 total update 01000: train_loss=3.6556 last_lr=1.00E-04 peak_cuda_mem=66.95GB bsz=120.0 n_tokens=10213312.0 
2025-05-22 19:08:36 | INFO | seamlessm4t.trainer | Evaluation Step 4...
2025-05-22 19:09:58 | INFO | seamlessm4t.trainer | Eval after 1000 updates: tot_loss=3.6219 best_loss=3.6219 patience_steps_left=10
2025-05-22 19:09:58 | INFO | seamlessm4t.trainer | Saving model
2025-05-22 19:18:20 | INFO | seamlessm4t.trainer | Epoch 002 (686 steps) / update 364 total update 01050: train_loss=3.6598 last_lr=9.76E-05 peak_cuda_mem=65.34GB bsz=120.0 n_tokens=10191603.2 
2025-05-22 19:26:28 | INFO | seamlessm4t.trainer | Epoch 002 (686 steps) / update 414 total update 01100: train_loss=3.6470 last_lr=9.53E-05 peak_cuda_mem=53.15GB bsz=120.0 n_tokens=10044620.8 
2025-05-22 19:34:31 | INFO | seamlessm4t.trainer | Epoch 002 (686 steps) / update 464 total update 01150: train_loss=3.6264 last_lr=9.33E-05 peak_cuda_mem=52.27GB bsz=120.0 n_tokens=10030528.0 
2025-05-22 19:42:37 | INFO | seamlessm4t.trainer | Epoch 002 (686 steps) / update 514 total update 01200: train_loss=3.6119 last_lr=9.13E-05 peak_cuda_mem=48.07GB bsz=120.0 n_tokens=10143872.0 
2025-05-22 19:50:45 | INFO | seamlessm4t.trainer | Epoch 002 (686 steps) / update 564 total update 01250: train_loss=3.6156 last_lr=8.94E-05 peak_cuda_mem=57.54GB bsz=120.0 n_tokens=10174374.4 
2025-05-22 19:50:45 | INFO | seamlessm4t.trainer | Evaluation Step 5...
2025-05-22 19:52:02 | INFO | seamlessm4t.trainer | Eval after 1250 updates: tot_loss=3.5799 best_loss=3.5799 patience_steps_left=10
2025-05-22 19:52:03 | INFO | seamlessm4t.trainer | Saving model
2025-05-22 20:00:28 | INFO | seamlessm4t.trainer | Epoch 002 (686 steps) / update 614 total update 01300: train_loss=3.6219 last_lr=8.77E-05 peak_cuda_mem=49.86GB bsz=120.0 n_tokens=10111744.0 
2025-05-22 20:08:37 | INFO | seamlessm4t.trainer | Epoch 002 (686 steps) / update 664 total update 01350: train_loss=3.6020 last_lr=8.61E-05 peak_cuda_mem=56.06GB bsz=120.0 n_tokens=10139379.2 
2025-05-22 20:16:53 | INFO | seamlessm4t.trainer | Epoch 003 (686 steps) / update 28 total update 01400: train_loss=3.5580 last_lr=8.45E-05 peak_cuda_mem=54.76GB bsz=120.0 n_tokens=10150563.2 
2025-05-22 20:25:11 | INFO | seamlessm4t.trainer | Epoch 003 (686 steps) / update 78 total update 01450: train_loss=3.5511 last_lr=8.30E-05 peak_cuda_mem=66.94GB bsz=120.0 n_tokens=10081574.4 
2025-05-22 20:33:31 | INFO | seamlessm4t.trainer | Epoch 003 (686 steps) / update 128 total update 01500: train_loss=3.5336 last_lr=8.16E-05 peak_cuda_mem=56.78GB bsz=120.0 n_tokens=10083942.4 
2025-05-22 20:33:31 | INFO | seamlessm4t.trainer | Evaluation Step 6...
2025-05-22 20:34:52 | INFO | seamlessm4t.trainer | Eval after 1500 updates: tot_loss=3.5539 best_loss=3.5539 patience_steps_left=10
2025-05-22 20:34:52 | INFO | seamlessm4t.trainer | Saving model
2025-05-22 20:43:25 | INFO | seamlessm4t.trainer | Epoch 003 (686 steps) / update 178 total update 01550: train_loss=3.5487 last_lr=8.03E-05 peak_cuda_mem=46.77GB bsz=120.0 n_tokens=10146368.0 
2025-05-22 20:51:38 | INFO | seamlessm4t.trainer | Epoch 003 (686 steps) / update 228 total update 01600: train_loss=3.5245 last_lr=7.91E-05 peak_cuda_mem=61.76GB bsz=120.0 n_tokens=10052172.8 
2025-05-22 20:59:49 | INFO | seamlessm4t.trainer | Epoch 003 (686 steps) / update 278 total update 01650: train_loss=3.5459 last_lr=7.78E-05 peak_cuda_mem=52.08GB bsz=120.0 n_tokens=9977011.2 
2025-05-22 21:08:03 | INFO | seamlessm4t.trainer | Epoch 003 (686 steps) / update 328 total update 01700: train_loss=3.5398 last_lr=7.67E-05 peak_cuda_mem=56.75GB bsz=120.0 n_tokens=10189235.2 
2025-05-22 21:16:18 | INFO | seamlessm4t.trainer | Epoch 003 (686 steps) / update 378 total update 01750: train_loss=3.5479 last_lr=7.56E-05 peak_cuda_mem=53.29GB bsz=120.0 n_tokens=10233088.0 
2025-05-22 21:16:18 | INFO | seamlessm4t.trainer | Evaluation Step 7...
2025-05-22 21:17:42 | INFO | seamlessm4t.trainer | Eval after 1750 updates: tot_loss=3.5371 best_loss=3.5371 patience_steps_left=10
2025-05-22 21:17:42 | INFO | seamlessm4t.trainer | Saving model
2025-05-22 21:26:07 | INFO | seamlessm4t.trainer | Epoch 003 (686 steps) / update 428 total update 01800: train_loss=3.5296 last_lr=7.45E-05 peak_cuda_mem=66.74GB bsz=120.0 n_tokens=10194854.4 
2025-05-22 21:34:13 | INFO | seamlessm4t.trainer | Epoch 003 (686 steps) / update 478 total update 01850: train_loss=3.5325 last_lr=7.35E-05 peak_cuda_mem=58.65GB bsz=120.0 n_tokens=9986803.2 
2025-05-22 21:42:26 | INFO | seamlessm4t.trainer | Epoch 003 (686 steps) / update 528 total update 01900: train_loss=3.5158 last_lr=7.25E-05 peak_cuda_mem=53.83GB bsz=120.0 n_tokens=10186560.0 
2025-05-22 21:50:38 | INFO | seamlessm4t.trainer | Epoch 003 (686 steps) / update 578 total update 01950: train_loss=3.5300 last_lr=7.16E-05 peak_cuda_mem=66.22GB bsz=120.0 n_tokens=10116672.0 
2025-05-22 21:58:53 | INFO | seamlessm4t.trainer | Epoch 003 (686 steps) / update 628 total update 02000: train_loss=3.5190 last_lr=7.07E-05 peak_cuda_mem=58.33GB bsz=120.0 n_tokens=10158067.2 
2025-05-22 21:58:53 | INFO | seamlessm4t.trainer | Evaluation Step 8...
2025-05-22 22:00:13 | INFO | seamlessm4t.trainer | Eval after 2000 updates: tot_loss=3.5207 best_loss=3.5207 patience_steps_left=10
2025-05-22 22:00:13 | INFO | seamlessm4t.trainer | Saving model
2025-05-22 22:08:51 | INFO | seamlessm4t.trainer | Epoch 003 (686 steps) / update 678 total update 02050: train_loss=3.5159 last_lr=6.98E-05 peak_cuda_mem=57.55GB bsz=120.0 n_tokens=10305830.4 
2025-05-22 22:17:09 | INFO | seamlessm4t.trainer | Epoch 004 (686 steps) / update 42 total update 02100: train_loss=3.4716 last_lr=6.90E-05 peak_cuda_mem=49.05GB bsz=120.0 n_tokens=10090643.2 
2025-05-22 22:25:30 | INFO | seamlessm4t.trainer | Epoch 004 (686 steps) / update 92 total update 02150: train_loss=3.4655 last_lr=6.82E-05 peak_cuda_mem=64.96GB bsz=120.0 n_tokens=10104448.0 
2025-05-22 22:33:52 | INFO | seamlessm4t.trainer | Epoch 004 (686 steps) / update 142 total update 02200: train_loss=3.4824 last_lr=6.74E-05 peak_cuda_mem=48.06GB bsz=120.0 n_tokens=10132364.8 
2025-05-22 22:42:14 | INFO | seamlessm4t.trainer | Epoch 004 (686 steps) / update 192 total update 02250: train_loss=3.4672 last_lr=6.67E-05 peak_cuda_mem=66.21GB bsz=120.0 n_tokens=10138252.8 
2025-05-22 22:42:15 | INFO | seamlessm4t.trainer | Evaluation Step 9...
2025-05-22 22:43:35 | INFO | seamlessm4t.trainer | Eval after 2250 updates: tot_loss=3.5067 best_loss=3.5067 patience_steps_left=10
2025-05-22 22:43:35 | INFO | seamlessm4t.trainer | Saving model
2025-05-22 22:52:12 | INFO | seamlessm4t.trainer | Epoch 004 (686 steps) / update 242 total update 02300: train_loss=3.4639 last_lr=6.59E-05 peak_cuda_mem=64.97GB bsz=120.0 n_tokens=10125862.4 
2025-05-22 23:00:27 | INFO | seamlessm4t.trainer | Epoch 004 (686 steps) / update 292 total update 02350: train_loss=3.4676 last_lr=6.52E-05 peak_cuda_mem=58.61GB bsz=120.0 n_tokens=10124480.0 
2025-05-22 23:08:40 | INFO | seamlessm4t.trainer | Epoch 004 (686 steps) / update 342 total update 02400: train_loss=3.4705 last_lr=6.45E-05 peak_cuda_mem=50.09GB bsz=120.0 n_tokens=10097638.4 
2025-05-22 23:16:53 | INFO | seamlessm4t.trainer | Epoch 004 (686 steps) / update 392 total update 02450: train_loss=3.4711 last_lr=6.39E-05 peak_cuda_mem=54.13GB bsz=120.0 n_tokens=10139289.6 
2025-05-22 23:25:08 | INFO | seamlessm4t.trainer | Epoch 004 (686 steps) / update 442 total update 02500: train_loss=3.4790 last_lr=6.32E-05 peak_cuda_mem=57.63GB bsz=120.0 n_tokens=10219481.6 
2025-05-22 23:25:09 | INFO | seamlessm4t.trainer | Evaluation Step 10...
2025-05-22 23:26:27 | INFO | seamlessm4t.trainer | Eval after 2500 updates: tot_loss=3.4961 best_loss=3.4961 patience_steps_left=10
2025-05-22 23:26:27 | INFO | seamlessm4t.trainer | Saving model
2025-05-22 23:34:55 | INFO | seamlessm4t.trainer | Epoch 004 (686 steps) / update 492 total update 02550: train_loss=3.4798 last_lr=6.26E-05 peak_cuda_mem=54.56GB bsz=120.0 n_tokens=10148569.6 
2025-05-22 23:43:02 | INFO | seamlessm4t.trainer | Epoch 004 (686 steps) / update 542 total update 02600: train_loss=3.4667 last_lr=6.20E-05 peak_cuda_mem=48.28GB bsz=120.0 n_tokens=10038899.2 
2025-05-22 23:51:13 | INFO | seamlessm4t.trainer | Epoch 004 (686 steps) / update 592 total update 02650: train_loss=3.4653 last_lr=6.14E-05 peak_cuda_mem=60.20GB bsz=120.0 n_tokens=10066598.4 
2025-05-22 23:59:26 | INFO | seamlessm4t.trainer | Epoch 004 (686 steps) / update 642 total update 02700: train_loss=3.4596 last_lr=6.09E-05 peak_cuda_mem=57.51GB bsz=120.0 n_tokens=10138252.8 
2025-05-23 00:07:43 | INFO | seamlessm4t.trainer | Epoch 005 (686 steps) / update 6 total update 02750: train_loss=3.4608 last_lr=6.03E-05 peak_cuda_mem=53.60GB bsz=120.0 n_tokens=10235971.2 
2025-05-23 00:07:43 | INFO | seamlessm4t.trainer | Evaluation Step 11...
2025-05-23 00:09:02 | INFO | seamlessm4t.trainer | Eval after 2750 updates: tot_loss=3.4967 best_loss=3.4961 patience_steps_left=9
2025-05-23 00:17:30 | INFO | seamlessm4t.trainer | Epoch 005 (686 steps) / update 56 total update 02800: train_loss=3.4134 last_lr=5.98E-05 peak_cuda_mem=54.06GB bsz=120.0 n_tokens=10132096.0 
2025-05-23 00:25:55 | INFO | seamlessm4t.trainer | Epoch 005 (686 steps) / update 106 total update 02850: train_loss=3.4267 last_lr=5.92E-05 peak_cuda_mem=47.78GB bsz=120.0 n_tokens=10147852.8 
2025-05-23 00:34:21 | INFO | seamlessm4t.trainer | Epoch 005 (686 steps) / update 156 total update 02900: train_loss=3.4228 last_lr=5.87E-05 peak_cuda_mem=58.28GB bsz=120.0 n_tokens=10177587.2 
2025-05-23 00:42:44 | INFO | seamlessm4t.trainer | Epoch 005 (686 steps) / update 206 total update 02950: train_loss=3.4227 last_lr=5.82E-05 peak_cuda_mem=64.94GB bsz=120.0 n_tokens=10074176.0 
2025-05-23 00:51:10 | INFO | seamlessm4t.trainer | Epoch 005 (686 steps) / update 256 total update 03000: train_loss=3.4334 last_lr=5.77E-05 peak_cuda_mem=55.83GB bsz=120.0 n_tokens=10059136.0 
2025-05-23 00:51:10 | INFO | seamlessm4t.trainer | Evaluation Step 12...
2025-05-23 00:52:31 | INFO | seamlessm4t.trainer | Eval after 3000 updates: tot_loss=3.4896 best_loss=3.4896 patience_steps_left=10
2025-05-23 00:52:31 | INFO | seamlessm4t.trainer | Saving model
2025-05-23 01:01:07 | INFO | seamlessm4t.trainer | Epoch 005 (686 steps) / update 306 total update 03050: train_loss=3.4185 last_lr=5.73E-05 peak_cuda_mem=65.05GB bsz=120.0 n_tokens=10118579.2 
2025-05-23 01:09:26 | INFO | seamlessm4t.trainer | Epoch 005 (686 steps) / update 356 total update 03100: train_loss=3.4268 last_lr=5.68E-05 peak_cuda_mem=51.78GB bsz=120.0 n_tokens=10084800.0 
2025-05-23 01:17:49 | INFO | seamlessm4t.trainer | Epoch 005 (686 steps) / update 406 total update 03150: train_loss=3.4299 last_lr=5.63E-05 peak_cuda_mem=50.66GB bsz=120.0 n_tokens=10142668.8 
2025-05-23 01:26:12 | INFO | seamlessm4t.trainer | Epoch 005 (686 steps) / update 456 total update 03200: train_loss=3.4333 last_lr=5.59E-05 peak_cuda_mem=53.29GB bsz=120.0 n_tokens=10177459.2 
2025-05-23 01:34:40 | INFO | seamlessm4t.trainer | Epoch 005 (686 steps) / update 506 total update 03250: train_loss=3.4303 last_lr=5.55E-05 peak_cuda_mem=67.15GB bsz=120.0 n_tokens=10297766.4 
2025-05-23 01:34:40 | INFO | seamlessm4t.trainer | Evaluation Step 13...
2025-05-23 01:36:00 | INFO | seamlessm4t.trainer | Eval after 3250 updates: tot_loss=3.4788 best_loss=3.4788 patience_steps_left=10
2025-05-23 01:36:00 | INFO | seamlessm4t.trainer | Saving model
2025-05-23 01:44:30 | INFO | seamlessm4t.trainer | Epoch 005 (686 steps) / update 556 total update 03300: train_loss=3.4243 last_lr=5.50E-05 peak_cuda_mem=54.79GB bsz=120.0 n_tokens=10059276.8 
2025-05-23 01:52:49 | INFO | seamlessm4t.trainer | Epoch 005 (686 steps) / update 606 total update 03350: train_loss=3.4209 last_lr=5.46E-05 peak_cuda_mem=56.55GB bsz=120.0 n_tokens=10161318.4 
2025-05-23 02:01:03 | INFO | seamlessm4t.trainer | Epoch 005 (686 steps) / update 656 total update 03400: train_loss=3.4246 last_lr=5.42E-05 peak_cuda_mem=51.01GB bsz=120.0 n_tokens=10057446.4 
2025-05-23 02:09:23 | INFO | seamlessm4t.trainer | Epoch 006 (686 steps) / update 20 total update 03450: train_loss=3.3972 last_lr=5.38E-05 peak_cuda_mem=56.84GB bsz=120.0 n_tokens=10038438.4 
2025-05-23 02:17:52 | INFO | seamlessm4t.trainer | Epoch 006 (686 steps) / update 70 total update 03500: train_loss=3.3861 last_lr=5.35E-05 peak_cuda_mem=66.22GB bsz=120.0 n_tokens=10149286.4 
2025-05-23 02:17:52 | INFO | seamlessm4t.trainer | Evaluation Step 14...
2025-05-23 02:19:11 | INFO | seamlessm4t.trainer | Eval after 3500 updates: tot_loss=3.4808 best_loss=3.4788 patience_steps_left=9
2025-05-23 02:27:38 | INFO | seamlessm4t.trainer | Epoch 006 (686 steps) / update 120 total update 03550: train_loss=3.3884 last_lr=5.31E-05 peak_cuda_mem=53.85GB bsz=120.0 n_tokens=10144102.4 
2025-05-23 02:36:04 | INFO | seamlessm4t.trainer | Epoch 006 (686 steps) / update 170 total update 03600: train_loss=3.3978 last_lr=5.27E-05 peak_cuda_mem=46.77GB bsz=120.0 n_tokens=10027814.4 
2025-05-23 02:44:34 | INFO | seamlessm4t.trainer | Epoch 006 (686 steps) / update 220 total update 03650: train_loss=3.3844 last_lr=5.23E-05 peak_cuda_mem=58.31GB bsz=120.0 n_tokens=10106572.8 
2025-05-23 02:52:57 | INFO | seamlessm4t.trainer | Epoch 006 (686 steps) / update 270 total update 03700: train_loss=3.3913 last_lr=5.20E-05 peak_cuda_mem=58.27GB bsz=120.0 n_tokens=10018252.8 
2025-05-23 03:01:23 | INFO | seamlessm4t.trainer | Epoch 006 (686 steps) / update 320 total update 03750: train_loss=3.3919 last_lr=5.16E-05 peak_cuda_mem=63.33GB bsz=120.0 n_tokens=10208857.6 
2025-05-23 03:01:23 | INFO | seamlessm4t.trainer | Evaluation Step 15...
2025-05-23 03:02:45 | INFO | seamlessm4t.trainer | Eval after 3750 updates: tot_loss=3.4711 best_loss=3.4711 patience_steps_left=10
2025-05-23 03:02:45 | INFO | seamlessm4t.trainer | Saving model
2025-05-23 03:11:25 | INFO | seamlessm4t.trainer | Epoch 006 (686 steps) / update 370 total update 03800: train_loss=3.3927 last_lr=5.13E-05 peak_cuda_mem=50.03GB bsz=120.0 n_tokens=10182041.6 
2025-05-23 03:19:44 | INFO | seamlessm4t.trainer | Epoch 006 (686 steps) / update 420 total update 03850: train_loss=3.3907 last_lr=5.10E-05 peak_cuda_mem=57.63GB bsz=120.0 n_tokens=10163417.6 
2025-05-23 03:27:58 | INFO | seamlessm4t.trainer | Epoch 006 (686 steps) / update 470 total update 03900: train_loss=3.3869 last_lr=5.06E-05 peak_cuda_mem=48.53GB bsz=120.0 n_tokens=10154483.2 
2025-05-23 03:36:07 | INFO | seamlessm4t.trainer | Epoch 006 (686 steps) / update 520 total update 03950: train_loss=3.3854 last_lr=5.03E-05 peak_cuda_mem=56.85GB bsz=120.0 n_tokens=10050163.2 
2025-05-23 03:44:14 | INFO | seamlessm4t.trainer | Epoch 006 (686 steps) / update 570 total update 04000: train_loss=3.3899 last_lr=5.00E-05 peak_cuda_mem=54.05GB bsz=120.0 n_tokens=10061094.4 
2025-05-23 03:44:14 | INFO | seamlessm4t.trainer | Evaluation Step 16...
2025-05-23 03:45:32 | INFO | seamlessm4t.trainer | Eval after 4000 updates: tot_loss=3.4751 best_loss=3.4711 patience_steps_left=9
2025-05-23 03:54:00 | INFO | seamlessm4t.trainer | Epoch 006 (686 steps) / update 620 total update 04050: train_loss=3.3976 last_lr=4.97E-05 peak_cuda_mem=59.47GB bsz=120.0 n_tokens=10152870.4 
2025-05-23 04:02:25 | INFO | seamlessm4t.trainer | Epoch 006 (686 steps) / update 670 total update 04100: train_loss=3.3873 last_lr=4.94E-05 peak_cuda_mem=61.73GB bsz=120.0 n_tokens=10173273.6 
2025-05-23 04:10:54 | INFO | seamlessm4t.trainer | Epoch 007 (686 steps) / update 34 total update 04150: train_loss=3.3626 last_lr=4.91E-05 peak_cuda_mem=68.05GB bsz=120.0 n_tokens=10240083.2 
2025-05-23 04:19:17 | INFO | seamlessm4t.trainer | Epoch 007 (686 steps) / update 84 total update 04200: train_loss=3.3578 last_lr=4.88E-05 peak_cuda_mem=61.32GB bsz=120.0 n_tokens=10010560.0 
2025-05-23 04:27:51 | INFO | seamlessm4t.trainer | Epoch 007 (686 steps) / update 134 total update 04250: train_loss=3.3565 last_lr=4.85E-05 peak_cuda_mem=57.90GB bsz=120.0 n_tokens=10251660.8 
2025-05-23 04:27:51 | INFO | seamlessm4t.trainer | Evaluation Step 17...
2025-05-23 04:29:11 | INFO | seamlessm4t.trainer | Eval after 4250 updates: tot_loss=3.4820 best_loss=3.4711 patience_steps_left=8
2025-05-23 04:37:42 | INFO | seamlessm4t.trainer | Epoch 007 (686 steps) / update 184 total update 04300: train_loss=3.3566 last_lr=4.82E-05 peak_cuda_mem=59.59GB bsz=120.0 n_tokens=10146880.0 
2025-05-23 04:46:14 | INFO | seamlessm4t.trainer | Epoch 007 (686 steps) / update 234 total update 04350: train_loss=3.3591 last_lr=4.79E-05 peak_cuda_mem=56.29GB bsz=120.0 n_tokens=10201420.8 
2025-05-23 04:54:42 | INFO | seamlessm4t.trainer | Epoch 007 (686 steps) / update 284 total update 04400: train_loss=3.3538 last_lr=4.77E-05 peak_cuda_mem=53.43GB bsz=120.0 n_tokens=10058713.6 
2025-05-23 05:03:06 | INFO | seamlessm4t.trainer | Epoch 007 (686 steps) / update 334 total update 04450: train_loss=3.3613 last_lr=4.74E-05 peak_cuda_mem=49.08GB bsz=120.0 n_tokens=10079616.0 
2025-05-23 05:11:33 | INFO | seamlessm4t.trainer | Epoch 007 (686 steps) / update 384 total update 04500: train_loss=3.3737 last_lr=4.71E-05 peak_cuda_mem=56.84GB bsz=120.0 n_tokens=10097011.2 
2025-05-23 05:11:34 | INFO | seamlessm4t.trainer | Evaluation Step 18...
2025-05-23 05:12:53 | INFO | seamlessm4t.trainer | Eval after 4500 updates: tot_loss=3.4712 best_loss=3.4711 patience_steps_left=7
2025-05-23 05:21:21 | INFO | seamlessm4t.trainer | Epoch 007 (686 steps) / update 434 total update 04550: train_loss=3.3612 last_lr=4.69E-05 peak_cuda_mem=54.28GB bsz=120.0 n_tokens=10116262.4 
2025-05-23 05:29:50 | INFO | seamlessm4t.trainer | Epoch 007 (686 steps) / update 484 total update 04600: train_loss=3.3527 last_lr=4.66E-05 peak_cuda_mem=63.55GB bsz=120.0 n_tokens=10160768.0 
2025-05-23 05:38:18 | INFO | seamlessm4t.trainer | Epoch 007 (686 steps) / update 534 total update 04650: train_loss=3.3598 last_lr=4.64E-05 peak_cuda_mem=49.39GB bsz=120.0 n_tokens=10172096.0 
2025-05-23 05:46:45 | INFO | seamlessm4t.trainer | Epoch 007 (686 steps) / update 584 total update 04700: train_loss=3.3562 last_lr=4.61E-05 peak_cuda_mem=49.84GB bsz=120.0 n_tokens=10150592.0 
2025-05-23 05:55:09 | INFO | seamlessm4t.trainer | Epoch 007 (686 steps) / update 634 total update 04750: train_loss=3.3604 last_lr=4.59E-05 peak_cuda_mem=57.59GB bsz=120.0 n_tokens=9979916.8 
2025-05-23 05:55:09 | INFO | seamlessm4t.trainer | Evaluation Step 19...
2025-05-23 05:56:28 | INFO | seamlessm4t.trainer | Eval after 4750 updates: tot_loss=3.4686 best_loss=3.4686 patience_steps_left=10
2025-05-23 05:56:28 | INFO | seamlessm4t.trainer | Saving model
2025-05-23 06:05:06 | INFO | seamlessm4t.trainer | Epoch 007 (686 steps) / update 684 total update 04800: train_loss=3.3542 last_lr=4.56E-05 peak_cuda_mem=58.72GB bsz=120.0 n_tokens=10109747.2 
2025-05-23 06:13:28 | INFO | seamlessm4t.trainer | Epoch 008 (686 steps) / update 48 total update 04850: train_loss=3.3262 last_lr=4.54E-05 peak_cuda_mem=58.34GB bsz=120.0 n_tokens=10042598.4 
2025-05-23 06:21:58 | INFO | seamlessm4t.trainer | Epoch 008 (686 steps) / update 98 total update 04900: train_loss=3.3259 last_lr=4.52E-05 peak_cuda_mem=55.50GB bsz=120.0 n_tokens=10143168.0 
2025-05-23 06:30:28 | INFO | seamlessm4t.trainer | Epoch 008 (686 steps) / update 148 total update 04950: train_loss=3.3306 last_lr=4.49E-05 peak_cuda_mem=64.39GB bsz=120.0 n_tokens=10220774.4 
2025-05-23 06:38:55 | INFO | seamlessm4t.trainer | Epoch 008 (686 steps) / update 198 total update 05000: train_loss=3.3350 last_lr=4.47E-05 peak_cuda_mem=53.82GB bsz=120.0 n_tokens=10107699.2 
2025-05-23 06:38:55 | INFO | seamlessm4t.trainer | Evaluation Step 20...
2025-05-23 06:40:13 | INFO | seamlessm4t.trainer | Eval after 5000 updates: tot_loss=3.4733 best_loss=3.4686 patience_steps_left=9
2025-05-23 06:48:40 | INFO | seamlessm4t.trainer | Epoch 008 (686 steps) / update 248 total update 05050: train_loss=3.3269 last_lr=4.45E-05 peak_cuda_mem=67.12GB bsz=120.0 n_tokens=10163558.4 
2025-05-23 06:57:04 | INFO | seamlessm4t.trainer | Epoch 008 (686 steps) / update 298 total update 05100: train_loss=3.3317 last_lr=4.43E-05 peak_cuda_mem=55.84GB bsz=120.0 n_tokens=10067520.0 
2025-05-23 07:05:33 | INFO | seamlessm4t.trainer | Epoch 008 (686 steps) / update 348 total update 05150: train_loss=3.3382 last_lr=4.41E-05 peak_cuda_mem=56.27GB bsz=120.0 n_tokens=10133132.8 
2025-05-23 07:14:03 | INFO | seamlessm4t.trainer | Epoch 008 (686 steps) / update 398 total update 05200: train_loss=3.3356 last_lr=4.39E-05 peak_cuda_mem=65.07GB bsz=120.0 n_tokens=10116006.4 
2025-05-23 07:22:32 | INFO | seamlessm4t.trainer | Epoch 008 (686 steps) / update 448 total update 05250: train_loss=3.3330 last_lr=4.36E-05 peak_cuda_mem=55.46GB bsz=120.0 n_tokens=10183948.8 
2025-05-23 07:22:32 | INFO | seamlessm4t.trainer | Evaluation Step 21...
2025-05-23 07:23:52 | INFO | seamlessm4t.trainer | Eval after 5250 updates: tot_loss=3.4751 best_loss=3.4686 patience_steps_left=8
2025-05-23 07:32:17 | INFO | seamlessm4t.trainer | Epoch 008 (686 steps) / update 498 total update 05300: train_loss=3.3314 last_lr=4.34E-05 peak_cuda_mem=51.78GB bsz=120.0 n_tokens=10121344.0 
2025-05-23 07:40:33 | INFO | seamlessm4t.trainer | Epoch 008 (686 steps) / update 548 total update 05350: train_loss=3.3317 last_lr=4.32E-05 peak_cuda_mem=58.31GB bsz=120.0 n_tokens=10159168.0 
2025-05-23 07:48:47 | INFO | seamlessm4t.trainer | Epoch 008 (686 steps) / update 598 total update 05400: train_loss=3.3401 last_lr=4.30E-05 peak_cuda_mem=45.87GB bsz=120.0 n_tokens=10079052.8 
2025-05-23 07:57:01 | INFO | seamlessm4t.trainer | Epoch 008 (686 steps) / update 648 total update 05450: train_loss=3.3396 last_lr=4.28E-05 peak_cuda_mem=51.10GB bsz=120.0 n_tokens=10161676.8 
2025-05-23 08:05:20 | INFO | seamlessm4t.trainer | Epoch 009 (686 steps) / update 12 total update 05500: train_loss=3.3274 last_lr=4.26E-05 peak_cuda_mem=49.31GB bsz=120.0 n_tokens=10123033.6 
2025-05-23 08:05:20 | INFO | seamlessm4t.trainer | Evaluation Step 22...
2025-05-23 08:06:41 | INFO | seamlessm4t.trainer | Eval after 5500 updates: tot_loss=3.4783 best_loss=3.4686 patience_steps_left=7
2025-05-23 08:14:59 | INFO | seamlessm4t.trainer | Epoch 009 (686 steps) / update 62 total update 05550: train_loss=3.2996 last_lr=4.24E-05 peak_cuda_mem=61.75GB bsz=120.0 n_tokens=10135155.2 
2025-05-23 08:23:19 | INFO | seamlessm4t.trainer | Epoch 009 (686 steps) / update 112 total update 05600: train_loss=3.3050 last_lr=4.23E-05 peak_cuda_mem=63.16GB bsz=120.0 n_tokens=10030438.4 
2025-05-23 08:31:39 | INFO | seamlessm4t.trainer | Epoch 009 (686 steps) / update 162 total update 05650: train_loss=3.3035 last_lr=4.21E-05 peak_cuda_mem=47.64GB bsz=120.0 n_tokens=10136115.2 
2025-05-23 08:39:55 | INFO | seamlessm4t.trainer | Epoch 009 (686 steps) / update 212 total update 05700: train_loss=3.3087 last_lr=4.19E-05 peak_cuda_mem=54.08GB bsz=120.0 n_tokens=10022336.0 
2025-05-23 08:48:15 | INFO | seamlessm4t.trainer | Epoch 009 (686 steps) / update 262 total update 05750: train_loss=3.3084 last_lr=4.17E-05 peak_cuda_mem=55.04GB bsz=120.0 n_tokens=10138726.4 
2025-05-23 08:48:15 | INFO | seamlessm4t.trainer | Evaluation Step 23...
2025-05-23 08:49:39 | INFO | seamlessm4t.trainer | Eval after 5750 updates: tot_loss=3.4817 best_loss=3.4686 patience_steps_left=6
2025-05-23 08:57:58 | INFO | seamlessm4t.trainer | Epoch 009 (686 steps) / update 312 total update 05800: train_loss=3.3114 last_lr=4.15E-05 peak_cuda_mem=48.94GB bsz=120.0 n_tokens=10093030.4 
2025-05-23 09:06:13 | INFO | seamlessm4t.trainer | Epoch 009 (686 steps) / update 362 total update 05850: train_loss=3.3068 last_lr=4.13E-05 peak_cuda_mem=53.60GB bsz=120.0 n_tokens=10055833.6 
2025-05-23 09:14:33 | INFO | seamlessm4t.trainer | Epoch 009 (686 steps) / update 412 total update 05900: train_loss=3.3090 last_lr=4.12E-05 peak_cuda_mem=60.22GB bsz=120.0 n_tokens=10208192.0 
2025-05-23 09:22:54 | INFO | seamlessm4t.trainer | Epoch 009 (686 steps) / update 462 total update 05950: train_loss=3.3075 last_lr=4.10E-05 peak_cuda_mem=66.99GB bsz=120.0 n_tokens=10179302.4 
2025-05-23 09:31:13 | INFO | seamlessm4t.trainer | Epoch 009 (686 steps) / update 512 total update 06000: train_loss=3.3047 last_lr=4.08E-05 peak_cuda_mem=56.76GB bsz=120.0 n_tokens=10077990.4 
2025-05-23 09:31:13 | INFO | seamlessm4t.trainer | Evaluation Step 24...
2025-05-23 09:32:37 | INFO | seamlessm4t.trainer | Eval after 6000 updates: tot_loss=3.4824 best_loss=3.4686 patience_steps_left=5
2025-05-23 09:40:57 | INFO | seamlessm4t.trainer | Epoch 009 (686 steps) / update 562 total update 06050: train_loss=3.3146 last_lr=4.07E-05 peak_cuda_mem=48.99GB bsz=120.0 n_tokens=10172979.2 
2025-05-23 09:49:19 | INFO | seamlessm4t.trainer | Epoch 009 (686 steps) / update 612 total update 06100: train_loss=3.3117 last_lr=4.05E-05 peak_cuda_mem=55.56GB bsz=120.0 n_tokens=10162726.4 
2025-05-23 09:57:35 | INFO | seamlessm4t.trainer | Epoch 009 (686 steps) / update 662 total update 06150: train_loss=3.3120 last_lr=4.03E-05 peak_cuda_mem=48.88GB bsz=120.0 n_tokens=10099084.8 
2025-05-23 10:05:50 | INFO | seamlessm4t.trainer | Epoch 010 (686 steps) / update 26 total update 06200: train_loss=3.2988 last_lr=4.02E-05 peak_cuda_mem=61.77GB bsz=120.0 n_tokens=10085158.4 
2025-05-23 10:14:13 | INFO | seamlessm4t.trainer | Epoch 010 (686 steps) / update 76 total update 06250: train_loss=3.2800 last_lr=4.00E-05 peak_cuda_mem=56.07GB bsz=120.0 n_tokens=10246297.6 
2025-05-23 10:14:14 | INFO | seamlessm4t.trainer | Evaluation Step 25...
2025-05-23 10:15:41 | INFO | seamlessm4t.trainer | Eval after 6250 updates: tot_loss=3.4885 best_loss=3.4686 patience_steps_left=4
2025-05-23 10:24:00 | INFO | seamlessm4t.trainer | Epoch 010 (686 steps) / update 126 total update 06300: train_loss=3.2761 last_lr=3.98E-05 peak_cuda_mem=52.46GB bsz=120.0 n_tokens=10121190.4 
2025-05-23 10:32:16 | INFO | seamlessm4t.trainer | Epoch 010 (686 steps) / update 176 total update 06350: train_loss=3.2847 last_lr=3.97E-05 peak_cuda_mem=49.68GB bsz=120.0 n_tokens=10051635.2 
2025-05-23 10:40:41 | INFO | seamlessm4t.trainer | Epoch 010 (686 steps) / update 226 total update 06400: train_loss=3.2859 last_lr=3.95E-05 peak_cuda_mem=65.43GB bsz=120.0 n_tokens=10212940.8 
2025-05-23 10:48:59 | INFO | seamlessm4t.trainer | Epoch 010 (686 steps) / update 276 total update 06450: train_loss=3.2862 last_lr=3.94E-05 peak_cuda_mem=53.19GB bsz=120.0 n_tokens=10169305.6 
2025-05-23 10:57:21 | INFO | seamlessm4t.trainer | Epoch 010 (686 steps) / update 326 total update 06500: train_loss=3.2826 last_lr=3.92E-05 peak_cuda_mem=54.61GB bsz=120.0 n_tokens=10231462.4 
2025-05-23 10:57:21 | INFO | seamlessm4t.trainer | Evaluation Step 26...
2025-05-23 10:58:46 | INFO | seamlessm4t.trainer | Eval after 6500 updates: tot_loss=3.4852 best_loss=3.4686 patience_steps_left=3
2025-05-23 11:07:07 | INFO | seamlessm4t.trainer | Epoch 010 (686 steps) / update 376 total update 06550: train_loss=3.2866 last_lr=3.91E-05 peak_cuda_mem=60.70GB bsz=120.0 n_tokens=10090496.0 
2025-05-23 11:15:25 | INFO | seamlessm4t.trainer | Epoch 010 (686 steps) / update 426 total update 06600: train_loss=3.2879 last_lr=3.89E-05 peak_cuda_mem=63.23GB bsz=120.0 n_tokens=10137113.6 
2025-05-23 11:23:44 | INFO | seamlessm4t.trainer | Epoch 010 (686 steps) / update 476 total update 06650: train_loss=3.2825 last_lr=3.88E-05 peak_cuda_mem=66.94GB bsz=120.0 n_tokens=10160192.0 
2025-05-23 11:32:02 | INFO | seamlessm4t.trainer | Epoch 010 (686 steps) / update 526 total update 06700: train_loss=3.2950 last_lr=3.86E-05 peak_cuda_mem=55.06GB bsz=120.0 n_tokens=10154905.6 
2025-05-23 11:40:20 | INFO | seamlessm4t.trainer | Epoch 010 (686 steps) / update 576 total update 06750: train_loss=3.2868 last_lr=3.85E-05 peak_cuda_mem=47.84GB bsz=120.0 n_tokens=10071603.2 
2025-05-23 11:40:20 | INFO | seamlessm4t.trainer | Evaluation Step 27...
2025-05-23 11:41:48 | INFO | seamlessm4t.trainer | Eval after 6750 updates: tot_loss=3.4844 best_loss=3.4686 patience_steps_left=2
2025-05-23 11:49:57 | INFO | seamlessm4t.trainer | Epoch 010 (686 steps) / update 626 total update 06800: train_loss=3.2939 last_lr=3.83E-05 peak_cuda_mem=51.08GB bsz=120.0 n_tokens=9943692.8 
2025-05-23 11:58:08 | INFO | seamlessm4t.trainer | Epoch 010 (686 steps) / update 676 total update 06850: train_loss=3.2851 last_lr=3.82E-05 peak_cuda_mem=61.39GB bsz=120.0 n_tokens=10205465.6 
2025-05-23 11:59:45 | INFO | seamlessm4t.trainer | Evaluation Step 27...
2025-05-23 12:01:09 | INFO | seamlessm4t.trainer | Eval after 6860 updates: tot_loss=3.4854 best_loss=3.4686 patience_steps_left=1 
